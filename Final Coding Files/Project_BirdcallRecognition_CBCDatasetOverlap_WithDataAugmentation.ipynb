{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project_BirdcallRecognition_CBCDatasetOverlap_WithDataAugmentation.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1nURW5U5ZgaKFs-Pq3Lp6tDiwba6jttkk","authorship_tag":"ABX9TyOAZYe3xi+Bz0jM70YbNxa8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Installing audiomentations library\n","!pip install audiomentations"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Wnzr6WHtgAd","executionInfo":{"status":"ok","timestamp":1647027427598,"user_tz":300,"elapsed":10499,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"ca879d78-b63a-46a2-8e1c-fbe7b0f4dff4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting audiomentations\n","  Downloading audiomentations-0.23.0-py3-none-any.whl (65 kB)\n","\u001b[?25l\r\u001b[K     |█████                           | 10 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 30 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 40 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 51 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 61 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 65 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: librosa<0.10.0,>0.7.2 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (0.8.1)\n","Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.21.5)\n","Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.4.1)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.1.0)\n","Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.10.3.post1)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (2.1.9)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (21.3)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.51.2)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.0.2)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (4.4.2)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.2.2)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (57.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.7)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.4.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.24.3)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<0.10.0,>0.7.2->audiomentations) (3.1.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (2.21)\n","Installing collected packages: audiomentations\n","Successfully installed audiomentations-0.23.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4130aWZjkjQL"},"outputs":[],"source":["import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import keras\n","import librosa\n","import librosa.display\n","import IPython.display as ipd\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from keras.applications.vgg16 import VGG16, preprocess_input\n","from keras.applications.resnet import preprocess_input, ResNet50, ResNet101\n","from keras.applications.densenet import preprocess_input, DenseNet121\n","from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n","from keras.models import Model, Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Input, Dropout, GlobalAveragePooling2D, Lambda, BatchNormalization, concatenate\n","from tensorflow.keras.callbacks.experimental import BackupAndRestore\n","from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score\n","from keras.models import load_model\n","from sklearn.decomposition import PCA\n","import pickle as pkl\n","from audiomentations import Compose, AddGaussianSNR, AddGaussianNoise, TimeStretch, PitchShift\n","from sklearn.preprocessing import MultiLabelBinarizer"]},{"cell_type":"markdown","source":["Data Loading, Splitting and Saving"],"metadata":{"id":"5VC5gokEpu7E"}},{"cell_type":"code","source":["# Loading CSV file of audio paths and labels\n","statFeatures = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Final_OverlapData_StatFeatures_Nocall_Labels.csv\")\n","\n","# Filtering out nocall labels audios\n","statData = statFeatures.drop(statFeatures.index[statFeatures['VGGCustStat'] == 'Nocall'])\n","\n","# Taking only filtered audio path and classlabels \n","statData1 = statData.loc[:, 'AudioPath']\n","Stat_labels = statData.loc[:, 'VGGCustStat']\n","\n","statData1 = statData1[:325952]\n","Stat_labels = Stat_labels[:325952]\n","\n","print(\"Statistical Data Shape after Nocall Removal = {}\".format(statData1.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQa-oRRjl2lO","executionInfo":{"status":"ok","timestamp":1646492769572,"user_tz":300,"elapsed":30465,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"efb16a40-dd6d-4331-d2a9-ab812db938f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Statistical Data Shape after Nocall Removal = (325952,)\n"]}]},{"cell_type":"code","source":["# Combining audio path and classlabel columns and converted into numpy array\n","data = pd.concat([statData1, Stat_labels], axis=1)\n","data = np.array(data)\n","print(data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBoT67xM9jE0","executionInfo":{"status":"ok","timestamp":1646492769575,"user_tz":300,"elapsed":27,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"a5ddad9e-619f-49f1-80c6-170452f52a85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(325952, 2)\n"]}]},{"cell_type":"markdown","source":["Feature Extraction for Noise Augmentated Data"],"metadata":{"id":"YxkHy67v9exu"}},{"cell_type":"code","source":["# Function for adding gaussian noise and timestretch using audiomentations library \n","def augmentation():\n","  transforms = Compose(\n","      [\n","          AddGaussianSNR(max_SNR=0.05, p=0.5),\n","          AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n","          TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n","      ]\n","  )\n","  return transforms"],"metadata":{"id":"VGcopMops3uZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initializing augmentation function\n","augment = augmentation()"],"metadata":{"id":"Ju91UQXn38aM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom batch generator which takes 64 chunks audios at a time and generates noise augmented mel-spectrograms and converts it into RGB channel and \n","# returns it with class label\n","def customBatchGenerator(data, augment):\n","  for i in range(0, len(data)//64):     \n","    batchX = np.zeros((64, 128, 431, 3), dtype='uint8')\n","    batchY = np.zeros((64, 1), dtype='U32')\n","    curchunk = 0\n","\n","    # Processing 64 chunk audios at a time\n","    for j in range(i*len(batchX), (i+1)*len(batchX)):\n","      # Laoding chunk audio\n","      x1, sr1 = librosa.load(data[j][0], sr=44100)\n","\n","      # Adding noise to loaded chunk audio\n","      augmentedX1 = augment(samples=x1, sample_rate=44100)\n","\n","      # Generating Mel-spectrogram\n","      mel = librosa.feature.melspectrogram(x1, 44100, n_mels=128, n_fft=1024, hop_length=512, fmin=20, fmax=16000)\n","      mel_scale = librosa.power_to_db(mel)\n","\n","      # Converting mel-spectrograms into RGB channel\n","      rgbData0 = np.stack([mel_scale, mel_scale, mel_scale], axis=-1)\n","      mean = rgbData0.mean()\n","      std = rgbData0.std()\n","      Xstd = (rgbData0 - mean) / (std + 1e-6)\n","      _min, _max = Xstd.min(), Xstd.max()\n","      norm_max = _max\n","      norm_min = _min\n","      if (_max - _min) > 1e-6:\n","          # Scale to [0, 255]\n","          V = Xstd\n","          V[V < norm_min] = norm_min\n","          V[V > norm_max] = norm_max\n","          V = 255 * (V - norm_min) / (norm_max - norm_min)\n","          batchX[curchunk] = V\n","      else:\n","          # Just zero\n","          V = np.zeros_like(Xstd, dtype=np.uint8)\n","          batchX[curchunk] = V\n","      batchY[curchunk] = data[j][1]\n","      curchunk = curchunk + 1\n","    yield batchX, batchY"],"metadata":{"id":"P_wgClRAP3gs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calling custom generator to process 64 chunk audios batch at a time\n","trainBatchGenerator = customBatchGenerator(data, augment)\n","\n","# Enumerating generator and saving noise augmented mel-spectrograms and class labels into numpy files \n","i = 0\n","for gendata in trainBatchGenerator:\n","  np.save(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Augmented Data/aug_vis_feat_\"+str(i)+\".npy\", gendata[0])\n","  np.save(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Labels/aug_vis_labels_\"+str(i)+\".npy\", gendata[1])\n","  i += 1"],"metadata":{"id":"ogu2NG2RTaiK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Deep Feature Extraction for Augmented Data\n"],"metadata":{"id":"jmhW88uETVRu"}},{"cell_type":"code","source":["# Custom generator to load files one by one\n","def customBatchLoaderGenerator(featureFiles, labelFiles, batchSize):    \n","  for i in range(batchSize):\n","    batchX = np.load(featureFiles[i])\n","    batchY = np.load(labelFiles[i])\n","\n","    yield batchX, batchY"],"metadata":{"id":"V6PGOCYSq-8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrieving file paths of augmented visual features and labels\n","featureFilePaths = os.listdir(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Augmented Data/\")\n","labelFilePaths = os.listdir(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Labels/\")\n","\n","for i in range(len(featureFilePaths)):\n","  featureFilePaths[i] = \"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Augmented Data/aug_vis_feat_\" + str(i) + \".npy\"\n","  labelFilePaths[i] = \"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Labels/aug_vis_labels_\" + str(i) + \".npy\"\n","\n","print(\"Total Files = {}\".format(len(featureFilePaths)))"],"metadata":{"id":"uBHzo71QrS-6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647027501160,"user_tz":300,"elapsed":66127,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"ca5be43b-9d45-4373-e7c8-45eac14e77cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Files = 5093\n"]}]},{"cell_type":"markdown","source":["ResNet50 (128, 431, 3)"],"metadata":{"id":"88cwdLWhqUNT"}},{"cell_type":"code","source":["# ResNet50 Model with pretrained ImageNet weights to extract deep features\n","\n","# Input Layer\n","input_layer = Input(shape = (128, 431, 3))\n","# Preprocess Layer\n","preprocess = Lambda(lambda x: preprocess_input(x), name='preprocess')(input_layer)\n","layer_1 = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(128, 431, 3))(preprocess)\n","# Global Average Pooling Layer\n","average = GlobalAveragePooling2D()(layer_1)\n","\n","resnetModel = Model(inputs=input_layer, outputs=average)\n","resnetModel.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s2GLyGclP6Rs","executionInfo":{"status":"ok","timestamp":1646495362793,"user_tz":300,"elapsed":13001,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"fff95767-ea10-4145-d5c2-bbc3db53806a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94773248/94765736 [==============================] - 4s 0us/step\n","94781440/94765736 [==============================] - 4s 0us/step\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 128, 431, 3)]     0         \n","                                                                 \n"," preprocess (Lambda)         (None, 128, 431, 3)       0         \n","                                                                 \n"," resnet50 (Functional)       (None, 4, 14, 2048)       23587712  \n","                                                                 \n"," global_average_pooling2d (G  (None, 2048)             0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n","=================================================================\n","Total params: 23,587,712\n","Trainable params: 23,534,592\n","Non-trainable params: 53,120\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["length = len(featureFilePaths)    # Variable to hold total number of files\n","\n","# Variables to store deep features and labels\n","features = np.zeros((length*64, 2048))\n","labels = np.zeros((length*64, 1), dtype='U32')\n","\n","# Calling custom generator to load files one by one\n","trainBatchGenerator = customBatchLoaderGenerator(featureFilePaths, labelFilePaths, length)\n","\n","# Enumerating generator and extracting deep features of various files one by one \n","for i, gendata in enumerate(trainBatchGenerator):\n","  if i==0:\n","    features = resnetModel.predict(gendata[0])\n","    labels = gendata[1]\n","  else:\n","    features = np.vstack([features, resnetModel.predict(gendata[0])])\n","    labels = np.vstack([labels, gendata[1]])"],"metadata":{"id":"BE0f0j9tqaQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving extracted deep features and labels into numpy files\n","np.save(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Deep Features_Augment/ResNet50_augmentFeatures_128.npy\", features)\n","np.save(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Deep Features_Augment/ResNet50_augmentLabels_128.npy\", labels)"],"metadata":{"id":"my6PoeP4a-W0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ResNet101 (128, 431, 3)"],"metadata":{"id":"iHQawIKdSiA2"}},{"cell_type":"code","source":["# ResNet101 Model with pretrained ImageNet weights to extract deep features\n","\n","# Input Layer\n","input_layer = Input(shape = (128, 431, 3))\n","# Preprocess Layer\n","preprocess = Lambda(lambda x: preprocess_input(x), name='preprocess')(input_layer)\n","layer_1 = ResNet101(include_top=False, weights=\"imagenet\", input_shape=(128, 431, 3))(preprocess)\n","# Global Average Pooling Layer\n","average = GlobalAveragePooling2D()(layer_1)\n","\n","resnetModel = Model(inputs=input_layer, outputs=average)\n","resnetModel.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Q-FFIRfSqQS","executionInfo":{"status":"ok","timestamp":1646588386150,"user_tz":300,"elapsed":9121,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"e31927b0-72f8-439c-a3f7-f7c5beb4ce9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n","171450368/171446536 [==============================] - 1s 0us/step\n","171458560/171446536 [==============================] - 1s 0us/step\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 128, 431, 3)]     0         \n","                                                                 \n"," preprocess (Lambda)         (None, 128, 431, 3)       0         \n","                                                                 \n"," resnet101 (Functional)      (None, 4, 14, 2048)       42658176  \n","                                                                 \n"," global_average_pooling2d (G  (None, 2048)             0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n","=================================================================\n","Total params: 42,658,176\n","Trainable params: 42,552,832\n","Non-trainable params: 105,344\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["length = len(featureFilePaths)    # Variable to hold total number of files\n","\n","# Variables to store deep features and labels\n","features = np.zeros((length*64, 2048))\n","labels = np.zeros((length*64, 1), dtype='U32')\n","\n","# Calling custom generator to load files one by one\n","trainBatchGenerator = customBatchLoaderGenerator(featureFilePaths, labelFilePaths, length)\n","\n","# Enumerating generator and extracting deep features of various files one by one \n","for i, gendata in enumerate(trainBatchGenerator):\n","  if i==0:\n","    features = resnetModel.predict(gendata[0])\n","    labels = gendata[1]\n","  else:\n","    features = np.vstack([features, resnetModel.predict(gendata[0])])\n","    labels = np.vstack([labels, gendata[1]])"],"metadata":{"id":"818vCrHHSwEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving extracted deep features into numpy file\n","np.save(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Deep Features_Augment/ResNet101_augmentFeatures_128.npy\", features)"],"metadata":{"id":"QIO_IF3KSymH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["VGG16 (128, 431, 3)"],"metadata":{"id":"J6wgtHN352-l"}},{"cell_type":"code","source":["# VGG16 Model with pretrained ImageNet weights to extract deep features\n","\n","# Input Layer\n","input_layer = Input(shape = (128, 431, 3))\n","# Preprocess Layer\n","preprocess = Lambda(lambda x: preprocess_input(x), name='preprocess')(input_layer)\n","layer_1 = VGG16(include_top=False, weights=\"imagenet\", input_shape=(128, 431, 3))(preprocess)\n","# Global Average Pooling Layer\n","average = GlobalAveragePooling2D()(layer_1)\n","\n","vggModel = Model(inputs=input_layer, outputs=average)\n","vggModel.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8GY1bHPF51x7","executionInfo":{"status":"ok","timestamp":1646941647098,"user_tz":300,"elapsed":4650,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"d35b2a5e-48bc-4cf0-a954-1581a9cf8cd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 0s 0us/step\n","58900480/58889256 [==============================] - 0s 0us/step\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 128, 431, 3)]     0         \n","                                                                 \n"," preprocess (Lambda)         (None, 128, 431, 3)       0         \n","                                                                 \n"," vgg16 (Functional)          (None, 4, 13, 512)        14714688  \n","                                                                 \n"," global_average_pooling2d (G  (None, 512)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["length = len(featureFilePaths)    # Variable to hold total number of files\n","\n","# Variables to store deep features and labels\n","features = np.zeros((length*64, 512))\n","labels = np.zeros((length*64, 1), dtype='U32')\n","\n","# Calling custom generator to load files one by one\n","trainBatchGenerator = customBatchLoaderGenerator(featureFilePaths, labelFilePaths, length)\n","\n","# Enumerating generator and extracting deep features of various files one by one\n","for i, gendata in enumerate(trainBatchGenerator):\n","  if i==0:\n","    features = vggModel.predict(gendata)\n","    labels = gendata[1]\n","  else:\n","    features = np.vstack([features, vggModel.predict(gendata)])\n","    labels = np.vstack([labels, gendata[1]])"],"metadata":{"id":"mmIMpJhD6Cz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving extracted deep features into numpy file\n","np.save(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Deep Features_Augment/VGG16_augmentFeatures_128.npy\", features)"],"metadata":{"id":"SJAHP-Ed6QZl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DenseNet121 (128, 431, 3)"],"metadata":{"id":"o6seOlfreNvT"}},{"cell_type":"code","source":["# DenseNet121 Model with pretrained ImageNet weights to extract deep features\n","\n","# Input Layer\n","input_layer = Input(shape = (128, 431, 3))\n","# Preprocess Layer\n","preprocess = Lambda(lambda x: preprocess_input(x), name='preprocess')(input_layer)\n","layer_1 = DenseNet121(include_top=False, weights=\"imagenet\", input_shape=(128, 431, 3))(preprocess)\n","# Global Average Pooling Layer\n","average = GlobalAveragePooling2D()(layer_1)\n","\n","densenetModel = Model(inputs=input_layer, outputs=average)\n","densenetModel.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647027530961,"user_tz":300,"elapsed":11717,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"4eedf299-dbec-4316-aba9-eb85e52cac66","id":"RNSg_cqQeWKc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n","29089792/29084464 [==============================] - 1s 0us/step\n","29097984/29084464 [==============================] - 1s 0us/step\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 128, 431, 3)]     0         \n","                                                                 \n"," preprocess (Lambda)         (None, 128, 431, 3)       0         \n","                                                                 \n"," densenet121 (Functional)    (None, 4, 13, 1024)       7037504   \n","                                                                 \n"," global_average_pooling2d (G  (None, 1024)             0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n","=================================================================\n","Total params: 7,037,504\n","Trainable params: 6,953,856\n","Non-trainable params: 83,648\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["length = len(featureFilePaths)    # Variable to hold total number of files\n","\n","# Variables to store deep features and labels\n","features = np.zeros((length*64, 1024))\n","labels = np.zeros((length*64, 1), dtype='U32')\n","\n","# Calling custom generator to load files one by one\n","trainBatchGenerator = customBatchLoaderGenerator(featureFilePaths, labelFilePaths, length)\n","\n","# Enumerating generator and extracting deep features of various files one by one \n","for i, gendata in enumerate(trainBatchGenerator):\n","  if i==0:\n","    features = densenetModel.predict(gendata)\n","    labels = gendata[1]\n","  else:\n","    features = np.vstack([features, densenetModel.predict(gendata)])\n","    labels = np.vstack([labels, gendata[1]])"],"metadata":{"id":"ueg4z-sZeWKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving extracted deep features into numpy file\n","np.save(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data Augmentation/Deep Features_Augment/DenseNet121_augmentFeatures_128.npy\", features)"],"metadata":{"id":"JhlGR7HZeWKm"},"execution_count":null,"outputs":[]}]}