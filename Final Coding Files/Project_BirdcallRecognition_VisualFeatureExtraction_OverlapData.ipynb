{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project_BirdcallRecognition_VisualFeatureExtraction_OverlapData.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1kKMpJbsUCgyE_QwfYvIR153A2raBWjOb","authorship_tag":"ABX9TyNkjz32D+e7LCzPQBoOimJI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"GpKs_DET1uai"},"source":["import os\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import soundfile as sf\n","import librosa.display\n","import IPython.display as ipd\n","from matplotlib import pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading CSV file of overlap chunk data path and classlabels\n","\n","audioPath_labels = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Final_OverlapData_audioPath_classLabels.csv\")\n","print(audioPath_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Drm0uYnlyQ5w","executionInfo":{"status":"ok","timestamp":1643828063772,"user_tz":300,"elapsed":1711,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"69673a1c-c672-4d42-e548-2138645ceef9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                AudioPath ClassLabels\n","0       /content/drive/MyDrive/Colab Notebooks/Fall 20...      aldfly\n","1       /content/drive/MyDrive/Colab Notebooks/Fall 20...      aldfly\n","2       /content/drive/MyDrive/Colab Notebooks/Fall 20...      aldfly\n","3       /content/drive/MyDrive/Colab Notebooks/Fall 20...      aldfly\n","4       /content/drive/MyDrive/Colab Notebooks/Fall 20...      aldfly\n","...                                                   ...         ...\n","389773  /content/drive/MyDrive/Colab Notebooks/Fall 20...      yetvir\n","389774  /content/drive/MyDrive/Colab Notebooks/Fall 20...      yetvir\n","389775  /content/drive/MyDrive/Colab Notebooks/Fall 20...      yetvir\n","389776  /content/drive/MyDrive/Colab Notebooks/Fall 20...      yetvir\n","389777  /content/drive/MyDrive/Colab Notebooks/Fall 20...      yetvir\n","\n","[389778 rows x 2 columns]\n"]}]},{"cell_type":"markdown","source":["**Visual Feature - Mel-Spectogram Extraction**"],"metadata":{"id":"5Kos7NuXPP_n"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAcOw7f3cdft","executionInfo":{"status":"ok","timestamp":1638300454659,"user_tz":300,"elapsed":7490320,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"e27a31fa-856d-4703-922c-58908687693a"},"source":["# Visual features for Overlap Data using abs() in power_db\n","errorFileId = []\n","vis_feat = np.zeros((10000, 128, 431), dtype = float)\n","\n","noChunks = 10000\n","sr = 44100  \n","\n","n_fft = 1024\n","hop_length = 512\n","\n","print(\"Chunks of 10,000 :\")\n","\n","for i in range(0, len(audioPath_labels[\"AudioPath\"])//noChunks):\n","\n","  curChunk = 0\n","  for id in range(i*noChunks,(i+1)*noChunks):\n","\n","    if (id%100 == 0 or id==(len(audioPath_labels)-1)):\n","      print(\"Processed Audios : \", id)\n","\n","    try:\n","      x1, sr1 = librosa.load(audioPath_labels[\"AudioPath\"][id], sr=sr)\n","\n","      # Apply Mel-spec\n","      mel = librosa.feature.melspectrogram(x1, sr, n_mels=128, n_fft=1024, hop_length=512, fmin=20, fmax=16000)\n","      # transforming to mel scale\n","      mel_scale = librosa.power_to_db(abs(mel))\n","\n","      # Append into .npy\n","      vis_feat[curChunk] = mel_scale\n","\n","    except Exception as e:\n","      print(e)\n","      errorFileId.append(id)\n","      print(\"File Path: \" + audioPath_labels[\"AudioPath\"][id])\n","\n","    curChunk = curChunk + 1\n","\n","  #Saving file\n","  a1 = str(i*noChunks)\n","  a2 = str((i+1)*noChunks)\n","  filename = \"vis_feat_\"+a1+\"_\"+a2+\".npy\"\n","  np.save('/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap/'+filename, vis_feat)\n","  print(\"Saved : \", i*noChunks, \" to \", (i+1)*noChunks)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Chunks of 10,000 :\n","Processed Audios :  360000\n","Processed Audios :  360100\n","Processed Audios :  360200\n","Processed Audios :  360300\n","Processed Audios :  360400\n","Processed Audios :  360500\n","Processed Audios :  360600\n","Processed Audios :  360700\n","Processed Audios :  360800\n","Processed Audios :  360900\n","Processed Audios :  361000\n","Processed Audios :  361100\n","Processed Audios :  361200\n","Processed Audios :  361300\n","Processed Audios :  361400\n","Processed Audios :  361500\n","Processed Audios :  361600\n","Processed Audios :  361700\n","Processed Audios :  361800\n","Processed Audios :  361900\n","Processed Audios :  362000\n","Processed Audios :  362100\n","Processed Audios :  362200\n","Processed Audios :  362300\n","Processed Audios :  362400\n","Processed Audios :  362500\n","Processed Audios :  362600\n","Processed Audios :  362700\n","Processed Audios :  362800\n","Processed Audios :  362900\n","Processed Audios :  363000\n","Processed Audios :  363100\n","Processed Audios :  363200\n","Processed Audios :  363300\n","Processed Audios :  363400\n","Processed Audios :  363500\n","Processed Audios :  363600\n","Processed Audios :  363700\n","Processed Audios :  363800\n","Processed Audios :  363900\n","Processed Audios :  364000\n","Processed Audios :  364100\n","Processed Audios :  364200\n","Processed Audios :  364300\n","Processed Audios :  364400\n","Processed Audios :  364500\n","Processed Audios :  364600\n","Processed Audios :  364700\n","Processed Audios :  364800\n","Processed Audios :  364900\n","Processed Audios :  365000\n","Processed Audios :  365100\n","Processed Audios :  365200\n","Processed Audios :  365300\n","Processed Audios :  365400\n","Processed Audios :  365500\n","Processed Audios :  365600\n","Processed Audios :  365700\n","Processed Audios :  365800\n","Processed Audios :  365900\n","Processed Audios :  366000\n","Processed Audios :  366100\n","Processed Audios :  366200\n","Processed Audios :  366300\n","Processed Audios :  366400\n","Processed Audios :  366500\n","Processed Audios :  366600\n","Processed Audios :  366700\n","Processed Audios :  366800\n","Processed Audios :  366900\n","Processed Audios :  367000\n","Processed Audios :  367100\n","Processed Audios :  367200\n","Processed Audios :  367300\n","Processed Audios :  367400\n","Processed Audios :  367500\n","Processed Audios :  367600\n","Processed Audios :  367700\n","Processed Audios :  367800\n","Processed Audios :  367900\n","Processed Audios :  368000\n","Processed Audios :  368100\n","Processed Audios :  368200\n","Processed Audios :  368300\n","Processed Audios :  368400\n","Processed Audios :  368500\n","Processed Audios :  368600\n","Processed Audios :  368700\n","Processed Audios :  368800\n","Processed Audios :  368900\n","Processed Audios :  369000\n","Processed Audios :  369100\n","Processed Audios :  369200\n","Processed Audios :  369300\n","Processed Audios :  369400\n","Processed Audios :  369500\n","Processed Audios :  369600\n","Processed Audios :  369700\n","Processed Audios :  369800\n","Processed Audios :  369900\n","Saved :  360000  to  370000\n","Processed Audios :  370000\n","Processed Audios :  370100\n","Processed Audios :  370200\n","Processed Audios :  370300\n","Processed Audios :  370400\n","Processed Audios :  370500\n","Processed Audios :  370600\n","Processed Audios :  370700\n","Processed Audios :  370800\n","Processed Audios :  370900\n","Processed Audios :  371000\n","Processed Audios :  371100\n","Processed Audios :  371200\n","Processed Audios :  371300\n","Processed Audios :  371400\n","Processed Audios :  371500\n","Processed Audios :  371600\n","Processed Audios :  371700\n","Processed Audios :  371800\n","Processed Audios :  371900\n","Processed Audios :  372000\n","Processed Audios :  372100\n","Processed Audios :  372200\n","Processed Audios :  372300\n","Processed Audios :  372400\n","Processed Audios :  372500\n","Processed Audios :  372600\n","Processed Audios :  372700\n","Processed Audios :  372800\n","Processed Audios :  372900\n","Processed Audios :  373000\n","Processed Audios :  373100\n","Processed Audios :  373200\n","Processed Audios :  373300\n","Processed Audios :  373400\n","Processed Audios :  373500\n","Processed Audios :  373600\n","Processed Audios :  373700\n","Processed Audios :  373800\n","Processed Audios :  373900\n","Processed Audios :  374000\n","Processed Audios :  374100\n","Processed Audios :  374200\n","Processed Audios :  374300\n","Processed Audios :  374400\n","Processed Audios :  374500\n","Processed Audios :  374600\n","Processed Audios :  374700\n","Processed Audios :  374800\n","Processed Audios :  374900\n","Processed Audios :  375000\n","Processed Audios :  375100\n","Processed Audios :  375200\n","Processed Audios :  375300\n","Processed Audios :  375400\n","Processed Audios :  375500\n","Processed Audios :  375600\n","Processed Audios :  375700\n","Processed Audios :  375800\n","Processed Audios :  375900\n","Processed Audios :  376000\n","Processed Audios :  376100\n","Processed Audios :  376200\n","Processed Audios :  376300\n","Processed Audios :  376400\n","Processed Audios :  376500\n","Processed Audios :  376600\n","Processed Audios :  376700\n","Processed Audios :  376800\n","Processed Audios :  376900\n","Processed Audios :  377000\n","Processed Audios :  377100\n","Processed Audios :  377200\n","Processed Audios :  377300\n","Processed Audios :  377400\n","Processed Audios :  377500\n","Processed Audios :  377600\n","Processed Audios :  377700\n","Processed Audios :  377800\n","Processed Audios :  377900\n","Processed Audios :  378000\n","Processed Audios :  378100\n","Processed Audios :  378200\n","Processed Audios :  378300\n","Processed Audios :  378400\n","Processed Audios :  378500\n","Processed Audios :  378600\n","Processed Audios :  378700\n","Processed Audios :  378800\n","Processed Audios :  378900\n","Processed Audios :  379000\n","Processed Audios :  379100\n","Processed Audios :  379200\n","Processed Audios :  379300\n","Processed Audios :  379400\n","Processed Audios :  379500\n","Processed Audios :  379600\n","Processed Audios :  379700\n","Processed Audios :  379800\n","Processed Audios :  379900\n","Saved :  370000  to  380000\n"]}]},{"cell_type":"markdown","source":["**Converting Mel-Spectograms to RGB**"],"metadata":{"id":"S4cqRq1xPKK4"}},{"cell_type":"code","source":["# Loading 10000 visual features one by one and converting them into RGB\n","for k in range(40):\n","  filename = \"vis_feat_\"+str(k*10000)+\"_\"+str((k+1)*10000)+\".npy\"\n","\n","  CBCData = np.load(\"/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap/\"+filename)\n","\n","  CBCDataRGB = np.zeros((5000,128,431,3), dtype='uint8')\n","\n","  # Splitting 10000 visual features into two separate files of 5000 visual features\n","  for j in range(2):\n","    curChunk = 0\n","\n","    # Converting visual features into RGB one by one \n","    for i in range(j*len(CBCDataRGB), (j+1)*len(CBCDataRGB)):\n","      rgbData0 = np.stack([CBCData[i], CBCData[i], CBCData[i]], axis=-1)\n","      mean = rgbData0.mean()\n","      std = rgbData0.std()\n","      Xstd = (rgbData0 - mean) / (std + 1e-6)\n","      _min, _max = Xstd.min(), Xstd.max()\n","      norm_max = _max\n","      norm_min = _min\n","      if (_max - _min) > 1e-6:\n","          # Scale to [0, 255]\n","          V = Xstd\n","          V[V < norm_min] = norm_min\n","          V[V > norm_max] = norm_max\n","          V = 255 * (V - norm_min) / (norm_max - norm_min)\n","          CBCDataRGB[curChunk] = V\n","      else:\n","          # Just zero\n","          V = np.zeros_like(Xstd, dtype=np.uint8)\n","          CBCDataRGB[curChunk] = V\n","      curChunk = curChunk + 1\n","\n","    # Saving file\n","    filename = \"vis_feat_\"+str((k*2)+j)+\".npy\"\n","    np.save('/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Data/'+filename, CBCDataRGB)\n","    print(\"Saved : \"+filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEaKeDDMUq5o","executionInfo":{"status":"ok","timestamp":1640454270669,"user_tz":300,"elapsed":6086742,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"b9460fc6-cf6e-45ee-8fe9-4c6659ca26c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved : vis_feat_0.npy\n","Saved : vis_feat_1.npy\n","Saved : vis_feat_2.npy\n","Saved : vis_feat_3.npy\n","Saved : vis_feat_4.npy\n","Saved : vis_feat_5.npy\n","Saved : vis_feat_6.npy\n","Saved : vis_feat_7.npy\n","Saved : vis_feat_8.npy\n","Saved : vis_feat_9.npy\n","Saved : vis_feat_10.npy\n","Saved : vis_feat_11.npy\n","Saved : vis_feat_12.npy\n","Saved : vis_feat_13.npy\n","Saved : vis_feat_14.npy\n","Saved : vis_feat_15.npy\n","Saved : vis_feat_16.npy\n","Saved : vis_feat_17.npy\n","Saved : vis_feat_18.npy\n","Saved : vis_feat_19.npy\n","Saved : vis_feat_20.npy\n","Saved : vis_feat_21.npy\n","Saved : vis_feat_22.npy\n","Saved : vis_feat_23.npy\n","Saved : vis_feat_24.npy\n","Saved : vis_feat_25.npy\n","Saved : vis_feat_26.npy\n","Saved : vis_feat_27.npy\n","Saved : vis_feat_28.npy\n","Saved : vis_feat_29.npy\n","Saved : vis_feat_30.npy\n","Saved : vis_feat_31.npy\n","Saved : vis_feat_32.npy\n","Saved : vis_feat_33.npy\n","Saved : vis_feat_34.npy\n","Saved : vis_feat_35.npy\n","Saved : vis_feat_36.npy\n","Saved : vis_feat_37.npy\n","Saved : vis_feat_38.npy\n","Saved : vis_feat_39.npy\n","Saved : vis_feat_40.npy\n","Saved : vis_feat_41.npy\n","Saved : vis_feat_42.npy\n","Saved : vis_feat_43.npy\n","Saved : vis_feat_44.npy\n","Saved : vis_feat_45.npy\n","Saved : vis_feat_46.npy\n","Saved : vis_feat_47.npy\n","Saved : vis_feat_48.npy\n","Saved : vis_feat_49.npy\n","Saved : vis_feat_50.npy\n","Saved : vis_feat_51.npy\n","Saved : vis_feat_52.npy\n","Saved : vis_feat_53.npy\n","Saved : vis_feat_54.npy\n","Saved : vis_feat_55.npy\n","Saved : vis_feat_56.npy\n","Saved : vis_feat_57.npy\n","Saved : vis_feat_58.npy\n","Saved : vis_feat_59.npy\n","Saved : vis_feat_60.npy\n","Saved : vis_feat_61.npy\n","Saved : vis_feat_62.npy\n","Saved : vis_feat_63.npy\n","Saved : vis_feat_64.npy\n","Saved : vis_feat_65.npy\n","Saved : vis_feat_66.npy\n","Saved : vis_feat_67.npy\n","Saved : vis_feat_68.npy\n","Saved : vis_feat_69.npy\n","Saved : vis_feat_70.npy\n","Saved : vis_feat_71.npy\n","Saved : vis_feat_72.npy\n","Saved : vis_feat_73.npy\n","Saved : vis_feat_74.npy\n","Saved : vis_feat_75.npy\n"]}]},{"cell_type":"markdown","source":["**One-Hot Encoding and Save labels**"],"metadata":{"id":"xIvBhKG41dRW"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import LabelEncoder\n","\n","labels = audioPath_labels['ClassLabels']\n","\n","# integer encode\n","label_encoder = LabelEncoder()\n","integer_encoded = label_encoder.fit_transform(labels)\n","\n","# binary encode\n","onehot_encoder = OneHotEncoder(sparse=False)\n","integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"],"metadata":{"id":"0pGCOgMIy3E1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save one-hot encoded labels\n","\n","for i in range((len(audioPath_labels)//5000)+1):\n","  if(i == 77):\n","    filename = \"labels_\"+str(i)+\".npy\"\n","    np.save('/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_Overlap_RGB/Labels/'+filename, onehot_encoded[i*5000:])\n","    np.save('/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Labels/'+filename, onehot_encoded[i*5000:])\n","    print(\"Saved : \"+filename)\n","  else:\n","    filename = \"labels_\"+str(i)+\".npy\"\n","    np.save('/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_Overlap_RGB/Labels/'+filename, onehot_encoded[i*5000:(i+1)*5000])\n","    np.save('/content/drive/MyDrive/Colab Notebooks/Fall 2021/Final Project/Visual Features_abs_Overlap_RGB/Labels/'+filename, onehot_encoded[i*5000:(i+1)*5000])\n","    print(\"Saved : \"+filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xpF236YVzGIt","executionInfo":{"status":"ok","timestamp":1640456444750,"user_tz":300,"elapsed":9372,"user":{"displayName":"Abhi Bhaveshkumar Amin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09984646343549019789"}},"outputId":"caecfed7-8ba1-4837-86d0-54239d9c7546"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved : labels_0.npy\n","Saved : labels_1.npy\n","Saved : labels_2.npy\n","Saved : labels_3.npy\n","Saved : labels_4.npy\n","Saved : labels_5.npy\n","Saved : labels_6.npy\n","Saved : labels_7.npy\n","Saved : labels_8.npy\n","Saved : labels_9.npy\n","Saved : labels_10.npy\n","Saved : labels_11.npy\n","Saved : labels_12.npy\n","Saved : labels_13.npy\n","Saved : labels_14.npy\n","Saved : labels_15.npy\n","Saved : labels_16.npy\n","Saved : labels_17.npy\n","Saved : labels_18.npy\n","Saved : labels_19.npy\n","Saved : labels_20.npy\n","Saved : labels_21.npy\n","Saved : labels_22.npy\n","Saved : labels_23.npy\n","Saved : labels_24.npy\n","Saved : labels_25.npy\n","Saved : labels_26.npy\n","Saved : labels_27.npy\n","Saved : labels_28.npy\n","Saved : labels_29.npy\n","Saved : labels_30.npy\n","Saved : labels_31.npy\n","Saved : labels_32.npy\n","Saved : labels_33.npy\n","Saved : labels_34.npy\n","Saved : labels_35.npy\n","Saved : labels_36.npy\n","Saved : labels_37.npy\n","Saved : labels_38.npy\n","Saved : labels_39.npy\n","Saved : labels_40.npy\n","Saved : labels_41.npy\n","Saved : labels_42.npy\n","Saved : labels_43.npy\n","Saved : labels_44.npy\n","Saved : labels_45.npy\n","Saved : labels_46.npy\n","Saved : labels_47.npy\n","Saved : labels_48.npy\n","Saved : labels_49.npy\n","Saved : labels_50.npy\n","Saved : labels_51.npy\n","Saved : labels_52.npy\n","Saved : labels_53.npy\n","Saved : labels_54.npy\n","Saved : labels_55.npy\n","Saved : labels_56.npy\n","Saved : labels_57.npy\n","Saved : labels_58.npy\n","Saved : labels_59.npy\n","Saved : labels_60.npy\n","Saved : labels_61.npy\n","Saved : labels_62.npy\n","Saved : labels_63.npy\n","Saved : labels_64.npy\n","Saved : labels_65.npy\n","Saved : labels_66.npy\n","Saved : labels_67.npy\n","Saved : labels_68.npy\n","Saved : labels_69.npy\n","Saved : labels_70.npy\n","Saved : labels_71.npy\n","Saved : labels_72.npy\n","Saved : labels_73.npy\n","Saved : labels_74.npy\n","Saved : labels_75.npy\n","Saved : labels_76.npy\n"]}]}]}